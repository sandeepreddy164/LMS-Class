# -*- coding: utf-8 -*-
"""Text_Summarization

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pZss_lpVcG29c7lLCH86efBOUFy-ANqB
"""

pip install transformers datasets torch nltk

from datasets import load_dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")
article = dataset['train'][0]['article']
print("Original Article:\n", article)

from transformers import BartForConditionalGeneration, BartTokenizer
tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

article = dataset["test"][0]["article"]
reference_summary = dataset["test"][0]["highlights"]
inputs = tokenizer(article, return_tensors="pt", truncation=True, max_length=1024)
summary_ids = model.generate(inputs["input_ids"], num_beams=4, min_length=30, max_length=150, early_stopping=True)
generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print("Reference Summary:\n", reference_summary)
print("\nGenerated Summary:\n", generated_summary)

pip install rouge-score

from rouge_score import rouge_scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
scores = scorer.score(reference_summary, generated_summary)
print("\nROUGE Scores:")
for key, value in scores.items():
    print(f"{key}: Precision: {value.precision:.4f}, Recall: {value.recall:.4f}, F1-score: {value.fmeasure:.4f}")

pip install bert-score

from bert_score import score
P, R, F1 = score([generated_summary], [reference_summary], lang="en", model_type="bert-base-uncased")
print("\nBERTScore:")
print(f"Precision: {P.mean():.2f}, Recall: {R.mean():.2f}, F1-score: {F1.mean():.2f}")

from transformers import T5Tokenizer, T5ForConditionalGeneration
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

text = """The T5 model is a transformer-based machine learning model developed by Google Research.
It is trained on a diverse dataset and performs multiple NLP tasks by converting them into text-to-text problems."""
input_text = "summarize: " + text
input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

summary_ids = model.generate(input_ids, max_length=50, num_beams=5, length_penalty=2.0, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
print(summary)

from datasets import load_dataset
dataset = load_dataset("cnn_dailymail", "3.0.0")
train_data = dataset["train"]
val_data = dataset["validation"]
test_data = dataset["test"]

