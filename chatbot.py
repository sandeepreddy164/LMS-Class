# -*- coding: utf-8 -*-
"""chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lyz4-WSP9RGT8pa9b1oz6xJyMQlvvMKM
"""

import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
model_name = "cardiffnlp/tweet-topic-21-multi"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
labels = [
    "arts_&_culture", "business_&_entrepreneurs", "celebrity_&_pop_culture", "diaries_&_daily_life",
    "family", "fashion_&_style", "film_tv_&_video", "fitness_&_health", "food_&_dining",
    "gaming", "learning_&_educational", "music", "news_&_social_concern", "other_hobbies",
    "relationships", "science_&_technology", "sports_&_esports", "travel_&_adventure",
    "youth_&_student_life"
]

texts=[
    "The latest iphone was just released with an incredible new camera",
    "Manchester United won their match with a stunning goal in the last minute.",
    "NASA just launched a new mission to explore the surface of Mars.",
    "The Oscars had some surprising winners this year!"
]
# Tokenize the input texts
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
# Perform classification
with torch.no_grad():
    outputs = model(**inputs)
# Convert logits to probabilities using softma
probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
# Get predicted labels
predictions = torch.argmax(probabilities, dim=1)
for text, pred, prob in zip(texts, predictions, probabilities):
    print(f"Text: {text}\nTopic: {labels[pred.item()]}, Confidence: {prob[pred].item():.4f}\n")

from transformers import pipeline
# Load the summarization pipeline
summarizer = pipeline("summarization")
# Input text to be summarized
text = """
Hugging Face is a company that specializes in natural language processing (NLP).
It has developed the Transformers library, which provides state-of-the-art models
for a wide range of NLP tasks such as text classification, information extraction,
question answering, summarization, translation, and more. The library is widely used
in both academia and industry due to its ease of use and flexibility.
"""

summary = summarizer(text, max_length=50, min_length=20, do_sample=False)
print("Summary:", summary[0]['summary_text'])

from transformers import AutoModelForCausalLM, AutoTokenizer
# Load a pre-trained text generation model and tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
# Define a prompt
prompt = "Once upon a time in a distant galaxy,"
# Encode the input text
inputs = tokenizer(prompt, return_tensors="pt")
# Generate text
output = model.generate(**inputs, max_length=50, num_return_sequences=1, temperature=0.7, top_k=50)
# Decode and print the generated text
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print(generated_text)

import os
import atexit
import shutil
from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

# Step 1: Load the pre-trained BlenderBot model and tokenizer
model_name = "facebook/blenderbot-1B-distill"

# Increase the timeout for downloading the tokenizer
tokenizer = BlenderbotTokenizer.from_pretrained(model_name, timeout=60)  # Increased timeout to 60 seconds

model = BlenderbotForConditionalGeneration.from_pretrained(model_name)

