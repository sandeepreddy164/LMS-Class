# -*- coding: utf-8 -*-
"""FakeNewsDetection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nFL2e-E1EztpdBN-7JnJNnmQU0eLmJQR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/fake_news.csv')
data.head()

data.shape

data.info()

data.isna().sum()

data = data.drop(['id'], axis = 1)

data = data.fillna('')

data['content'] = data['author']+' '+data['title']+' '+data['text']

data = data.drop(['author', 'title', 'text'], axis=1)

data.head()

# Convert to lowercase
data['content'] = data['content'].apply(lambda x: " ".join(x.lower() for x in x.split()))

data['content'] = data['content'].str.replace('[^\w\s]','')

from nltk.corpus import stopwords
stop  = stopwords.words('english')
data['content'] = data['content'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

from nltk.stem import WordNetLemmatizer
from textblob import Word
data['content'] = data['content'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
data['content'].head()

X=data['content']
y=data['label']

from sklearn.model_selection import train_test_split

# splitting into training and testing data
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3, random_state=45, stratify=y)

#validate the shape of train and test dataset
print (X_train.shape)
print (y_train.shape)
print (X_test.shape)
print (y_test.shape)

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000)
tfidf_vect.fit(data['content'])
xtrain_tfidf = tfidf_vect.transform(X_train)
xtest_tfidf = tfidf_vect.transform(X_test)

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn import metrics
pclf = PassiveAggressiveClassifier()
pclf.fit(xtrain_tfidf, y_train)
predictions = pclf.predict(xtest_tfidf)
print(metrics.classification_report(y_test, predictions))

print(metrics.confusion_matrix(y_test,predictions))

from sklearn.neural_network import MLPClassifier
mlpclf = MLPClassifier(hidden_layer_sizes=(256,64,16),
                       activation = 'relu',
                       solver = 'adam')
mlpclf.fit(xtrain_tfidf, y_train)
predictions = mlpclf.predict(xtest_tfidf)
print(metrics.classification_report(y_test, predictions))

print(metrics.confusion_matrix(y_test,predic7tions))

import pickle
pickle.dump(mlpclf, open("fakenews1.pkl", "wb"))

# load the model from disk
loaded_model = pickle.load(open("fakenews1.pkl", 'rb'))
# Transform X_test using the same TfidfVectorizer used during training
X_test_tfidf = tfidf_vect.transform(X_test)
result = loaded_model.score(X_test_tfidf, y_test)
print(result)

